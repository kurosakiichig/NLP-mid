{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNwPfhbQys3DDcAmiIkf+qX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurosakiichig/SW-mid/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXAQ4whLKqwR",
        "outputId": "a9a3007e-c75c-45ed-af28-1712701ae1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Apr 23 13:39 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "NBA 推文情感分析 & 文本检索\n",
        "依赖：pandas, numpy, scikit-learn, gensim, joblib, scipy\n",
        "数据文件：NBADataset.csv（放在与脚本同目录 或 /content）\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re, os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "import joblib, scipy.sparse\n",
        "\n",
        "# ====================== 1. 读数 & 预处理 ======================\n",
        "FILE = 'NBADataset.csv'                      # 修改路径即可\n",
        "df = pd.read_csv(FILE)\n",
        "\n",
        "# 简易英文停用词表\n",
        "_STOP = set(\"\"\"\n",
        "a about above after again against all am an and any are arent as at be because\n",
        "been before being below between both but by couldnt did didnt do does doesnt\n",
        "doing dont down during each few for from further had hadnt has hasnt have\n",
        "havent having he hed hell hes her here heres hers herself him himself his how\n",
        "hows i id ill im ive if in into is isnt it its itself lets me more most mustnt\n",
        "my myself no nor not of off on once only or other ought our ours ourselves out\n",
        "over own same shant she shed shell shes should shouldnt so some such than that\n",
        "thats the their theirs them themselves then there theres these they theyd theyll\n",
        "theyre theyve this those through to too under until up very was wasnt we wed\n",
        "well were weve were werent what whats when whens where wheres which while who\n",
        "whos whom why whys with wont would wouldnt you youd youll youre youve your\n",
        "yours yourself yourselves\n",
        "\"\"\".split())\n",
        "\n",
        "def clean(txt: str) -> str:\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r'https?://\\S+', '', txt)      # 去 URL\n",
        "    txt = re.sub(r'@\\w+', '', txt)              # 去 @mention\n",
        "    txt = re.sub(r'[^a-z\\s]', '', txt)          # 非字母→空\n",
        "    return ' '.join(t for t in txt.split() if t not in _STOP)\n",
        "\n",
        "df['clean_text'] = df['text'].astype(str).apply(clean)\n",
        "\n",
        "# 把 VADER polarity 映射为三分类标签\n",
        "df['sentiment'] = df['polarity'].apply(\n",
        "    lambda p: 'positive' if p > 0 else 'negative' if p < 0 else 'neutral'\n",
        ")\n",
        "\n",
        "# ====================== 2. 情感分析：TF-IDF + NB ======================\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    df['clean_text'], df['sentiment'],\n",
        "    test_size=0.2, random_state=42, stratify=df['sentiment']\n",
        ")\n",
        "\n",
        "clf_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=1_0000, ngram_range=(1,2))),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "clf_nb.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"\\n=== TF-IDF + NB  情感分类结果 ===\")\n",
        "print(classification_report(y_te, clf_nb.predict(X_te)))\n",
        "print(confusion_matrix(y_te, clf_nb.predict(X_te)))\n",
        "\n",
        "joblib.dump(clf_nb, 'tfidf_nb_sentiment.pkl')\n",
        "\n",
        "# ====================== 3. Skip-gram 语料训练 ======================\n",
        "sentences = [s.split() for s in df['clean_text']]\n",
        "sg_model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=5,\n",
        "    sg=1,              # 1 = Skip-gram\n",
        "    workers=4,\n",
        "    epochs=10\n",
        ")\n",
        "sg_model.save('skipgram.model')\n",
        "\n",
        "def sent_vec(tokens):\n",
        "    vecs = [sg_model.wv[w] for w in tokens if w in sg_model.wv]\n",
        "    return np.mean(vecs, axis=0) if vecs else np.zeros(sg_model.vector_size)\n",
        "\n",
        "# 为每篇推文生成文档向量（可缓存到硬盘）\n",
        "doc_vectors = np.vstack(df['clean_text'].apply(lambda x: sent_vec(x.split())))\n",
        "\n",
        "np.save('doc_vectors.npy', doc_vectors)        # 若需持久化\n",
        "\n",
        "# ====================== 4. 检索函数：Skip-gram 余弦相似度 ======================\n",
        "def retrieve(query: str, top_k=5):\n",
        "    q_vec = sent_vec(clean(query).split()).reshape(1, -1)\n",
        "    sims  = cosine_similarity(q_vec, doc_vectors).ravel()\n",
        "    idx   = sims.argsort()[::-1][:top_k]\n",
        "    return [(df.iloc[i]['text'], float(sims[i])) for i in idx]\n",
        "\n",
        "print(\"\\n=== Skip-gram 检索示例：\\\"lakers vs warriors\\\" ===\")\n",
        "for t, sc in retrieve(\"lakers vs warriors\"):\n",
        "    print(f\"{sc:.3f} | {t[:100]}...\")\n",
        "\n",
        "# ====================== 5. 组合示例 ======================\n",
        "def ensemble_sentiment(text):\n",
        "    \"\"\"NB 预测 + Skip-gram 最近邻投票（示范：正>负>中）\"\"\"\n",
        "    nb_pred = clf_nb.predict([clean(text)])[0]\n",
        "    # 找最近 10 条相似 tweet 的情感取众数\n",
        "    sims_idx = cosine_similarity(\n",
        "        sent_vec(clean(text).split()).reshape(1,-1),\n",
        "        doc_vectors\n",
        "    ).ravel().argsort()[::-1][:10]\n",
        "    neighbor_major = df.iloc[sims_idx]['sentiment'].mode()[0]\n",
        "    # 若两者一致，则输出；不一致按正>负>中优先\n",
        "    if nb_pred == neighbor_major:\n",
        "        return nb_pred\n",
        "    priority = ['positive', 'negative', 'neutral']\n",
        "    return min([nb_pred, neighbor_major], key=priority.index)\n",
        "\n",
        "print(\"\\nEnsemble demo →\", ensemble_sentiment(\"That game was unbelievable!\"))\n"
      ],
      "metadata": {
        "id": "UPh50uHlJJpD",
        "outputId": "b36f0d20-e36f-4a52-9640-b42144d5a935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'gensim'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-8538a2114af9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}