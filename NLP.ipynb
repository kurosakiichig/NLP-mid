{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME9unCRkVk63l8NR6kYBXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurosakiichig/SW-mid/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx4_Rs1BuyJ7",
        "outputId": "c9a7a6f6-041b-489b-85b9-c5260c7996d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Apr 11 13:37 .\n",
            "drwxr-xr-x 1 root root 4096 Apr 17 03:10 ..\n",
            "drwxr-xr-x 4 root root 4096 Apr 11 13:37 .config\n",
            "drwxr-xr-x 1 root root 4096 Apr 11 13:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å‡è®¾åŽŸæ–‡ä»¶åä¸º \"NBADataset - 12-07-2020 till 19-09-2020.csv\"\n",
        "!mv \"/content/NBADataset - 12-07-2020 till 19-09-2020.csv\" /content/NBADataset.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC_jWvp6uzq4",
        "outputId": "14c94a46-6292-4d21-8238-b0b3af330454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/NBADataset - 12-07-2020 till 19-09-2020.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import scipy.sparse\n",
        "import os\n",
        "\n",
        "# 1. åŠ è½½æ•°æ®ï¼ˆæ³¨æ„æ–‡ä»¶åéœ€ä¸Žä¸Šä¸€æ­¥ä¸€è‡´ï¼‰\n",
        "df = pd.read_csv('/NBADataset - 12-07-2020 till 19-09-2020.csv')\n",
        "\n",
        "# 2. å®šä¹‰åœç”¨è¯å¹¶æ¸…æ´—å‡½æ•°\n",
        "stop_words = set(\"a about above after again against all am an and any are arent as at be because been before being below between both but by couldnt did didnt do does doesnt doing dont down during each few for from further had hadnt has hasnt have havent having he hed hell hes her here heres hers herself him himself his how hows i id ill im ive if in into is isnt it its itself lets me more most mustnt my myself no nor not of off on once only or other ought our ours ourselves out over own same shant she shed shell shes should shouldnt so some such than that thats the their theirs them themselves then there theres these they theyd theyll theyre theyve this those through to too under until up very was wasnt we wed well were weve were werent what whats when whens where wheres which while who whos whom why whys with wont would wouldnt you youd youll youre youve your yours yourself yourselves\".split())\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    text = re.sub(r'@\\\\w+', '', text)\n",
        "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
        "    return ' '.join([t for t in text.split() if t not in stop_words])\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# 3. ç”Ÿæˆæƒ…æ„Ÿæ ‡ç­¾\n",
        "def to_sentiment(p):\n",
        "    if p > 0: return 'positive'\n",
        "    if p < 0: return 'negative'\n",
        "    return 'neutral'\n",
        "df['sentiment'] = df['polarity'].apply(to_sentiment)\n",
        "\n",
        "# 4. åˆ’åˆ†æ•°æ®é›†\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'], df['sentiment'],\n",
        "    test_size=0.2, random_state=42, stratify=df['sentiment']\n",
        ")\n",
        "\n",
        "# 5. æƒ…æ„Ÿåˆ†æžæ¨¡åž‹ï¼šTF-IDF + MultinomialNB\n",
        "sent_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "sent_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 6. è¯„ä¼°å¹¶ä¿å­˜æ¨¡åž‹\n",
        "y_pred = sent_pipeline.predict(X_test)\n",
        "print(\"=== æƒ…æ„Ÿåˆ†ç±»æŠ¥å‘Š ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"=== æ··æ·†çŸ©é˜µ ===\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "joblib.dump(sent_pipeline, 'nba_sentiment_model.pkl')\n",
        "print(\"æƒ…æ„Ÿæ¨¡åž‹å·²ä¿å­˜ï¼šnba_sentiment_model.pkl\")\n",
        "\n",
        "# 7. æž„å»º TF-IDF æ£€ç´¢ï¼ˆCosine ç›¸ä¼¼åº¦ï¼‰\n",
        "tfidf_vec = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "tfidf_matrix = tfidf_vec.fit_transform(df['clean_text'])\n",
        "joblib.dump(tfidf_vec, 'tfidf_vectorizer.pkl')\n",
        "scipy.sparse.save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
        "print(\"TF-IDF å‘é‡åŒ–å™¨ä¸ŽçŸ©é˜µå·²ä¿å­˜\")\n",
        "\n",
        "# 8. æ£€ç´¢å‡½æ•°ç¤ºä¾‹\n",
        "def retrieve_tfidf(query, top_k=5):\n",
        "    q_clean = clean_text(query)\n",
        "    q_vec = tfidf_vec.transform([q_clean])\n",
        "    sims = cosine_similarity(q_vec, tfidf_matrix).flatten()\n",
        "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
        "    return [(df.iloc[i]['text'], float(sims[i])) for i in top_idx]\n",
        "\n",
        "print(\"\\\\n=== æ£€ç´¢ç¤ºä¾‹ï¼š'lakers vs warriors' ===\")\n",
        "for text, sim in retrieve_tfidf(\"lakers vs warriors\"):\n",
        "    print(f\"Sim={sim:.3f} | {text[:100]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDZay9_Fu55C",
        "outputId": "e71a2bb5-4286-4b50-f7cf-b957b465b11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== æƒ…æ„Ÿåˆ†ç±»æŠ¥å‘Š ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.35      0.51      2302\n",
            "     neutral       0.66      1.00      0.80      9649\n",
            "    positive       1.00      0.55      0.71      7670\n",
            "\n",
            "    accuracy                           0.75     19621\n",
            "   macro avg       0.89      0.63      0.67     19621\n",
            "weighted avg       0.83      0.75      0.73     19621\n",
            "\n",
            "=== æ··æ·†çŸ©é˜µ ===\n",
            "[[ 795 1507    0]\n",
            " [   0 9649    0]\n",
            " [   0 3464 4206]]\n",
            "æƒ…æ„Ÿæ¨¡åž‹å·²ä¿å­˜ï¼šnba_sentiment_model.pkl\n",
            "TF-IDF å‘é‡åŒ–å™¨ä¸ŽçŸ©é˜µå·²ä¿å­˜\n",
            "\\n=== æ£€ç´¢ç¤ºä¾‹ï¼š'lakers vs warriors' ===\n",
            "Sim=0.000 | @NBA The @DetroitPistons could have had @spidadmitchell but passed on him for some lame player #detr...\n",
            "Sim=0.000 | Alberts Saturday Slate  Rockets -5 ðŸ”¨ Djokovich win US Open +125ðŸ”¨ Central Arkansas-5 ðŸ”¨ Cent Arkansas ...\n",
            "Sim=0.000 | #Nba #HoustonRockets #jamesharden #NBAPlayoffs  #OneMission Make it 3-2 win. Go rockets. H-Town fore...\n",
            "Sim=0.000 | I fully support everyone's right to protest against the issues they feel strongly about. The #Housto...\n",
            "Sim=0.000 | players on the #HoustonRockets  and #HoustonAstros right along by supporting #BlackLivesMatter focus...\n"
          ]
        }
      ]
    }
  ]
}