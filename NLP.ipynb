{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyME9unCRkVk63l8NR6kYBXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kurosakiichig/SW-mid/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zx4_Rs1BuyJ7",
        "outputId": "c9a7a6f6-041b-489b-85b9-c5260c7996d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Apr 11 13:37 .\n",
            "drwxr-xr-x 1 root root 4096 Apr 17 03:10 ..\n",
            "drwxr-xr-x 4 root root 4096 Apr 11 13:37 .config\n",
            "drwxr-xr-x 1 root root 4096 Apr 11 13:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 假设原文件名为 \"NBADataset - 12-07-2020 till 19-09-2020.csv\"\n",
        "!mv \"/content/NBADataset - 12-07-2020 till 19-09-2020.csv\" /content/NBADataset.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC_jWvp6uzq4",
        "outputId": "14c94a46-6292-4d21-8238-b0b3af330454"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/NBADataset - 12-07-2020 till 19-09-2020.csv': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import joblib\n",
        "import scipy.sparse\n",
        "import os\n",
        "\n",
        "# 1. 加载数据（注意文件名需与上一步一致）\n",
        "df = pd.read_csv('/NBADataset - 12-07-2020 till 19-09-2020.csv')\n",
        "\n",
        "# 2. 定义停用词并清洗函数\n",
        "stop_words = set(\"a about above after again against all am an and any are arent as at be because been before being below between both but by couldnt did didnt do does doesnt doing dont down during each few for from further had hadnt has hasnt have havent having he hed hell hes her here heres hers herself him himself his how hows i id ill im ive if in into is isnt it its itself lets me more most mustnt my myself no nor not of off on once only or other ought our ours ourselves out over own same shant she shed shell shes should shouldnt so some such than that thats the their theirs them themselves then there theres these they theyd theyll theyre theyve this those through to too under until up very was wasnt we wed well were weve were werent what whats when whens where wheres which while who whos whom why whys with wont would wouldnt you youd youll youre youve your yours yourself yourselves\".split())\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    text = re.sub(r'@\\\\w+', '', text)\n",
        "    text = re.sub(r'[^a-z\\\\s]', '', text)\n",
        "    return ' '.join([t for t in text.split() if t not in stop_words])\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)\n",
        "\n",
        "# 3. 生成情感标签\n",
        "def to_sentiment(p):\n",
        "    if p > 0: return 'positive'\n",
        "    if p < 0: return 'negative'\n",
        "    return 'neutral'\n",
        "df['sentiment'] = df['polarity'].apply(to_sentiment)\n",
        "\n",
        "# 4. 划分数据集\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'], df['sentiment'],\n",
        "    test_size=0.2, random_state=42, stratify=df['sentiment']\n",
        ")\n",
        "\n",
        "# 5. 情感分析模型：TF-IDF + MultinomialNB\n",
        "sent_pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "sent_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# 6. 评估并保存模型\n",
        "y_pred = sent_pipeline.predict(X_test)\n",
        "print(\"=== 情感分类报告 ===\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"=== 混淆矩阵 ===\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "joblib.dump(sent_pipeline, 'nba_sentiment_model.pkl')\n",
        "print(\"情感模型已保存：nba_sentiment_model.pkl\")\n",
        "\n",
        "# 7. 构建 TF-IDF 检索（Cosine 相似度）\n",
        "tfidf_vec = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "tfidf_matrix = tfidf_vec.fit_transform(df['clean_text'])\n",
        "joblib.dump(tfidf_vec, 'tfidf_vectorizer.pkl')\n",
        "scipy.sparse.save_npz('tfidf_matrix.npz', tfidf_matrix)\n",
        "print(\"TF-IDF 向量化器与矩阵已保存\")\n",
        "\n",
        "# 8. 检索函数示例\n",
        "def retrieve_tfidf(query, top_k=5):\n",
        "    q_clean = clean_text(query)\n",
        "    q_vec = tfidf_vec.transform([q_clean])\n",
        "    sims = cosine_similarity(q_vec, tfidf_matrix).flatten()\n",
        "    top_idx = np.argsort(sims)[::-1][:top_k]\n",
        "    return [(df.iloc[i]['text'], float(sims[i])) for i in top_idx]\n",
        "\n",
        "print(\"\\\\n=== 检索示例：'lakers vs warriors' ===\")\n",
        "for text, sim in retrieve_tfidf(\"lakers vs warriors\"):\n",
        "    print(f\"Sim={sim:.3f} | {text[:100]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDZay9_Fu55C",
        "outputId": "e71a2bb5-4286-4b50-f7cf-b957b465b11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 情感分类报告 ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.35      0.51      2302\n",
            "     neutral       0.66      1.00      0.80      9649\n",
            "    positive       1.00      0.55      0.71      7670\n",
            "\n",
            "    accuracy                           0.75     19621\n",
            "   macro avg       0.89      0.63      0.67     19621\n",
            "weighted avg       0.83      0.75      0.73     19621\n",
            "\n",
            "=== 混淆矩阵 ===\n",
            "[[ 795 1507    0]\n",
            " [   0 9649    0]\n",
            " [   0 3464 4206]]\n",
            "情感模型已保存：nba_sentiment_model.pkl\n",
            "TF-IDF 向量化器与矩阵已保存\n",
            "\\n=== 检索示例：'lakers vs warriors' ===\n",
            "Sim=0.000 | @NBA The @DetroitPistons could have had @spidadmitchell but passed on him for some lame player #detr...\n",
            "Sim=0.000 | Alberts Saturday Slate  Rockets -5 🔨 Djokovich win US Open +125🔨 Central Arkansas-5 🔨 Cent Arkansas ...\n",
            "Sim=0.000 | #Nba #HoustonRockets #jamesharden #NBAPlayoffs  #OneMission Make it 3-2 win. Go rockets. H-Town fore...\n",
            "Sim=0.000 | I fully support everyone's right to protest against the issues they feel strongly about. The #Housto...\n",
            "Sim=0.000 | players on the #HoustonRockets  and #HoustonAstros right along by supporting #BlackLivesMatter focus...\n"
          ]
        }
      ]
    }
  ]
}